{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import minimum_spanning_tree\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load MNIST data\n",
    "\n",
    "train_data = pd.read_csv('./mnist_train.csv', sep=',', header=None)\n",
    "train_labels = train_data[0]\n",
    "train_data = train_data.drop(0, axis=1)\n",
    "\n",
    "test_data = pd.read_csv('./mnist_test.csv', sep=',', header=None)\n",
    "test_labels = test_data[0]\n",
    "test_data = test_data.drop(0, axis=1)\n",
    "\n",
    "#separate data for generating graphs\n",
    "graph_data = train_data.sample(n = 10000, random_state=100)\n",
    "graph_labels = train_labels.sample(n = 10000, random_state=100)\n",
    "train_data = train_data.drop(graph_data.index)\n",
    "train_labels = train_labels.drop(graph_data.index)\n",
    "\n",
    "#convert data to pytorch tensors\n",
    "train_data = torch.FloatTensor(train_data.to_numpy())\n",
    "train_labels = torch.LongTensor(train_labels.to_numpy())\n",
    "test_data = torch.FloatTensor(test_data.to_numpy())\n",
    "graph_data = torch.FloatTensor(graph_data.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(50, 50) Vanilla FCN\n",
    "\n",
    "class Vanilla_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Vanilla_Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.lrelu1 = nn.LeakyReLU(0.01) #default negative slope\n",
    "        self.fc2 = nn.Linear(50, 50)\n",
    "        self.lrelu2 = nn.LeakyReLU(0.01) #default negative slope\n",
    "        self.fc3 = nn.Linear(50, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.lrelu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.lrelu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(net, optimizer, loss, epochs, train_data, train_labels, batch_size):\n",
    "    for i in range(epochs):\n",
    "        for j in range(0, train_data.shape[0], batch_size):\n",
    "            data_minibatch = Variable(train_data[j : j+batch_size])\n",
    "            label_minibatch = Variable(train_labels[j: j+batch_size])\n",
    "            optimizer.zero_grad()\n",
    "            net_out = net(data_minibatch)\n",
    "            net_loss = loss(net_out, label_minibatch)\n",
    "            net_loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(net, test_data, test_labels):\n",
    "    net_out = net(test_data)\n",
    "    test_out = torch.max(net_out.data, 1)[1].numpy()\n",
    "    return np.count_nonzero(test_out==test_labels) / len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ethan\\AppData\\Local\\Temp\\ipykernel_12060\\2732752644.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9487\n",
      "0.9479\n",
      "0.9511\n",
      "0.9519\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[417], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(my_net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate, momentum\u001b[38;5;241m=\u001b[39mmmt)\n\u001b[0;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m---> 14\u001b[0m \u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m cur_accuracy \u001b[38;5;241m=\u001b[39m test_accuracy(my_net, test_data, test_labels)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(cur_accuracy)\n",
      "Cell \u001b[1;32mIn[243], line 7\u001b[0m, in \u001b[0;36mSGD\u001b[1;34m(net, optimizer, loss, epochs, train_data, train_labels, batch_size)\u001b[0m\n\u001b[0;32m      5\u001b[0m label_minibatch \u001b[38;5;241m=\u001b[39m Variable(train_labels[j: j\u001b[38;5;241m+\u001b[39mbatch_size])\n\u001b[0;32m      6\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m----> 7\u001b[0m net_out \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_minibatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m net_loss \u001b[38;5;241m=\u001b[39m loss(net_out, label_minibatch)\n\u001b[0;32m      9\u001b[0m net_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[364], line 13\u001b[0m, in \u001b[0;36mVanilla_Net.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlrelu1(x)\n\u001b[0;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#determine optimal # of epochs for SGD\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 50 #typical value\n",
    "learning_rate = 0.0001 \n",
    "mmt = 0.9 #typical value\n",
    "cur_accuracy = 0\n",
    "prev_accuracy = 0\n",
    "while True:\n",
    "    prev_accuracy = cur_accuracy\n",
    "    my_net = Vanilla_Net()\n",
    "    optimizer = torch.optim.SGD(my_net.parameters(), lr=learning_rate, momentum=mmt)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    SGD(my_net, optimizer, loss, epochs, train_data, train_labels, batch_size)\n",
    "    cur_accuracy = test_accuracy(my_net, test_data, test_labels)\n",
    "    print(cur_accuracy)\n",
    "    if (cur_accuracy <= prev_accuracy-0.005):\n",
    "        break\n",
    "    epochs += 1\n",
    "epochs -= 1\n",
    "print(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above code a few times, it seems like the network typically achieves high accuracy after around 20 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join train and test data\n",
    "\n",
    "train_test_data = torch.cat((train_data, test_data))\n",
    "train_test_labels = torch.cat((train_labels, torch.LongTensor(test_labels.to_numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train vanilla network with train+test data\n",
    "\n",
    "def train_vanilla():\n",
    "    vanilla_net = Vanilla_Net()\n",
    "    optimizer = torch.optim.SGD(vanilla_net.parameters(), lr=0.0001, momentum=0.9)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    SGD(vanilla_net, optimizer, loss, epochs=20, train_data=train_test_data, train_labels=train_test_labels, batch_size=50)\n",
    "    return vanilla_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (50, 50) FCN trained with batch norm\n",
    "\n",
    "class BN_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BN_Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.bn1 = nn.BatchNorm1d(50)\n",
    "        self.lrelu1 = nn.LeakyReLU(0.01)\n",
    "        self.fc2 = nn.Linear(50, 50)\n",
    "        self.bn2 = nn.BatchNorm1d(50)\n",
    "        self.lrelu2 = nn.LeakyReLU(0.01)\n",
    "        self.fc3 = nn.Linear(50, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.lrelu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.lrelu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ethan\\AppData\\Local\\Temp\\ipykernel_24688\\1325394341.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(my_net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate, momentum\u001b[38;5;241m=\u001b[39mmmt)\n\u001b[0;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m---> 14\u001b[0m \u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m cur_accuracy \u001b[38;5;241m=\u001b[39m test_accuracy(my_net, test_data, test_labels)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(cur_accuracy)\n",
      "Cell \u001b[1;32mIn[73], line 9\u001b[0m, in \u001b[0;36mSGD\u001b[1;34m(net, optimizer, loss, epochs, train_data, train_labels, batch_size)\u001b[0m\n\u001b[0;32m      7\u001b[0m net_out \u001b[38;5;241m=\u001b[39m net(data_minibatch)\n\u001b[0;32m      8\u001b[0m net_loss \u001b[38;5;241m=\u001b[39m loss(net_out, label_minibatch)\n\u001b[1;32m----> 9\u001b[0m \u001b[43mnet_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#determine optimal # of epochs for batch_norm_net\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 50 #typical value\n",
    "learning_rate = 0.0001 \n",
    "mmt = 0.9 #typical value\n",
    "cur_accuracy = 0\n",
    "prev_accuracy = 0\n",
    "while True:\n",
    "    prev_accuracy = cur_accuracy\n",
    "    my_net = BN_Net()\n",
    "    optimizer = torch.optim.SGD(my_net.parameters(), lr=learning_rate, momentum=mmt)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    SGD(my_net, optimizer, loss, epochs, train_data, train_labels, batch_size)\n",
    "    cur_accuracy = test_accuracy(my_net, test_data, test_labels)\n",
    "    print(cur_accuracy)\n",
    "    if (cur_accuracy <= prev_accuracy-0.01):\n",
    "        break\n",
    "    epochs += 1\n",
    "epochs -= 1\n",
    "print(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch norm network achieves high accuracy after 20 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train batch norm network\n",
    "\n",
    "def train_batch_norm():\n",
    "    batch_norm_net = BN_Net()\n",
    "    optimizer = torch.optim.SGD(batch_norm_net.parameters(), lr=0.0001, momentum=0.9)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    SGD(batch_norm_net, optimizer, loss, epochs=20, train_data=train_test_data, train_labels=train_test_labels, batch_size=50)\n",
    "    return batch_norm_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ethan\\AppData\\Local\\Temp\\ipykernel_5308\\805662490.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n",
      "C:\\Users\\ethan\\AppData\\Local\\Temp\\ipykernel_5308\\1325394341.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    }
   ],
   "source": [
    "# train n networks for each strategy\n",
    "\n",
    "n_networks = 20\n",
    "vanilla_nets = []\n",
    "batch_norm_nets = []\n",
    "\n",
    "for i in range(n_networks):\n",
    "    vanilla_nets.append(train_vanilla())\n",
    "    batch_norm_nets.append(train_batch_norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract neuron outputs\n",
    "\n",
    "def neuron_values(net, data):\n",
    "    activations = []\n",
    "    def get_activation():\n",
    "        def hook(model, input, output):\n",
    "            activations.append(output.detach())\n",
    "        return hook\n",
    "    \n",
    "    net.lrelu1.register_forward_hook(get_activation())\n",
    "    net.lrelu2.register_forward_hook(get_activation())\n",
    "    net.eval()\n",
    "    net(data)\n",
    "    \n",
    "    activations[0] = activations[0].numpy()\n",
    "    activations[1] = activations[1].numpy()\n",
    "    neurons = np.concatenate((activations[0].T, activations[1].T))\n",
    "    return neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ethan\\AppData\\Local\\Temp\\ipykernel_5308\\805662490.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n",
      "C:\\Users\\ethan\\AppData\\Local\\Temp\\ipykernel_5308\\1325394341.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    }
   ],
   "source": [
    "vanilla_neurons = []\n",
    "batch_norm_neurons = []\n",
    "\n",
    "for i in range(n_networks):\n",
    "    vanilla_neurons.append(neuron_values(vanilla_nets[i], graph_data))\n",
    "    batch_norm_neurons.append(neuron_values(batch_norm_nets[i], graph_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_graph(neurons):\n",
    "    adj_matrix = abs(np.corrcoef(neurons))\n",
    "    np.fill_diagonal(adj_matrix, 0)\n",
    "    return adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split neuron data for one network into n subsets to construct n graphs\n",
    "\n",
    "n_subsets = 100\n",
    "\n",
    "vanilla_subsets = np.array_split(vanilla_neurons[0], n_subsets, 1)\n",
    "batch_norm_subsets = np.array_split(batch_norm_neurons[0], n_subsets, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct network graphs for subsets\n",
    "\n",
    "subset_network_graphs = []\n",
    "\n",
    "for i in range(n_subsets):\n",
    "    subset_network_graphs.append(correlation_graph(vanilla_subsets[i]))\n",
    "    subset_network_graphs.append(correlation_graph(batch_norm_subsets[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct network graphs for distinct networks\n",
    "\n",
    "network_graphs = []\n",
    "\n",
    "for i in range(n_networks):\n",
    "    network_graphs.append(correlation_graph(vanilla_neurons[i]))\n",
    "    network_graphs.append(correlation_graph(batch_norm_neurons[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topological clustering framework\n",
    "\n",
    "class TopClustering:\n",
    "    \"\"\"Topological clustering.\n",
    "    \n",
    "    Attributes:\n",
    "        n_clusters: \n",
    "          The number of clusters.\n",
    "        top_relative_weight:\n",
    "          Relative weight between the geometric and topological terms.\n",
    "          A floating point number between 0 and 1.\n",
    "        max_iter_alt:\n",
    "          Maximum number of iterations for the topological clustering.\n",
    "        max_iter_interp:\n",
    "          Maximum number of iterations for the topological interpolation.\n",
    "        learning_rate:\n",
    "          Learning rate for the topological interpolation.\n",
    "        \n",
    "    Reference:\n",
    "        Songdechakraiwut, Tananun, Bryan M. Krause, Matthew I. Banks, Kirill V. Nourski, and Barry D. Van Veen. \n",
    "        \"Fast topological clustering with Wasserstein distance.\" \n",
    "        International Conference on Learning Representations (ICLR). 2022.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, top_relative_weight, max_iter_alt,\n",
    "                 max_iter_interp, learning_rate):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.top_relative_weight = top_relative_weight\n",
    "        self.max_iter_alt = max_iter_alt\n",
    "        self.max_iter_interp = max_iter_interp\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def fit_predict(self, data):\n",
    "        \"\"\"Computes topological clustering and predicts cluster index for each sample.\n",
    "        \n",
    "            Args:\n",
    "                data:\n",
    "                  Training instances to cluster.\n",
    "                  \n",
    "            Returns:\n",
    "                Cluster index each sample belongs to.\n",
    "        \"\"\"\n",
    "        data = np.asarray(data)\n",
    "        n_node = data.shape[1]\n",
    "        n_edges = math.factorial(n_node) // math.factorial(2) // math.factorial(\n",
    "            n_node - 2)  # n_edges = (n_node choose 2)\n",
    "        n_births = n_node - 1\n",
    "        self.weight_array = np.append(\n",
    "            np.repeat(1 - self.top_relative_weight, n_edges),\n",
    "            np.repeat(self.top_relative_weight, n_edges))\n",
    "\n",
    "        # Networks represented as vectors concatenating geometric and topological info\n",
    "        X = []\n",
    "        for adj in data:\n",
    "            X.append(self._vectorize_geo_top_info(adj))\n",
    "        X = np.asarray(X)\n",
    "\n",
    "        # Random initial condition\n",
    "        self.centroids = X[random.sample(range(X.shape[0]), self.n_clusters)]\n",
    "\n",
    "        # Assign the nearest centroid index to each data point\n",
    "        assigned_centroids = self._get_nearest_centroid(\n",
    "            X[:, None, :], self.centroids[None, :, :])\n",
    "        prev_assigned_centroids = assigned_centroids\n",
    "\n",
    "        for it in range(self.max_iter_alt):\n",
    "            for cluster in range(self.n_clusters):\n",
    "                # Previous iteration centroid\n",
    "                prev_centroid = np.zeros((n_node, n_node))\n",
    "                prev_centroid[np.triu_indices(\n",
    "                    prev_centroid.shape[0],\n",
    "                    k=1)] = self.centroids[cluster][:n_edges]\n",
    "\n",
    "                # Determine data points belonging to each cluster\n",
    "                cluster_members = X[assigned_centroids == cluster]\n",
    "\n",
    "                # Compute the sample mean and top. centroid of the cluster\n",
    "                cc = cluster_members.mean(axis=0)\n",
    "                sample_mean = np.zeros((n_node, n_node))\n",
    "                sample_mean[np.triu_indices(sample_mean.shape[0],\n",
    "                                            k=1)] = cc[:n_edges]\n",
    "                top_centroid = cc[n_edges:]\n",
    "                top_centroid_birth_set = top_centroid[:n_births]\n",
    "                top_centroid_death_set = top_centroid[n_births:]\n",
    "\n",
    "                # Update the centroid\n",
    "                try:\n",
    "                    cluster_centroid = self._top_interpolation(\n",
    "                        prev_centroid, sample_mean, top_centroid_birth_set,\n",
    "                        top_centroid_death_set)\n",
    "                    self.centroids[cluster] = self._vectorize_geo_top_info(\n",
    "                        cluster_centroid)\n",
    "                except:\n",
    "                    print(\n",
    "                        'Error: Possibly due to the learning rate is not within appropriate range.'\n",
    "                    )\n",
    "                    sys.exit(1)\n",
    "\n",
    "            # Update the cluster membership\n",
    "            assigned_centroids = self._get_nearest_centroid(\n",
    "                X[:, None, :], self.centroids[None, :, :])\n",
    "\n",
    "            # Compute and print loss as it is progressively decreasing\n",
    "            loss = self._compute_top_dist(\n",
    "                X, self.centroids[assigned_centroids]).sum() / len(X)\n",
    "            #print('Iteration: %d -> Loss: %f' % (it, loss))\n",
    "\n",
    "            if (prev_assigned_centroids == assigned_centroids).all():\n",
    "                break\n",
    "            else:\n",
    "                prev_assigned_centroids = assigned_centroids\n",
    "        return assigned_centroids\n",
    "\n",
    "    def _vectorize_geo_top_info(self, adj):\n",
    "        birth_set, death_set = self._compute_birth_death_sets(\n",
    "            adj)  # topological info\n",
    "        vec = adj[np.triu_indices(adj.shape[0], k=1)]  # geometric info\n",
    "        return np.concatenate((vec, birth_set, death_set), axis=0)\n",
    "\n",
    "    def _compute_birth_death_sets(self, adj):\n",
    "        \"\"\"Computes birth and death sets of a network.\"\"\"\n",
    "        mst, nonmst = self._bd_demomposition(adj)\n",
    "        birth_ind = np.nonzero(mst)\n",
    "        death_ind = np.nonzero(nonmst)\n",
    "        return np.sort(mst[birth_ind]), np.sort(nonmst[death_ind])\n",
    "\n",
    "    def _bd_demomposition(self, adj):\n",
    "        \"\"\"Birth-death decomposition.\"\"\"\n",
    "        eps = np.nextafter(0, 1)\n",
    "        adj[adj == 0] = eps\n",
    "        adj = np.triu(adj, k=1)\n",
    "        Xcsr = csr_matrix(-adj)\n",
    "        Tcsr = minimum_spanning_tree(Xcsr)\n",
    "        mst = -Tcsr.toarray()  # reverse the negative sign\n",
    "        nonmst = adj - mst\n",
    "        birth_ind = np.nonzero(mst)\n",
    "        return mst, nonmst\n",
    "\n",
    "    def _get_nearest_centroid(self, X, centroids):\n",
    "        \"\"\"Determines cluster membership of data points.\"\"\"\n",
    "        dist = self._compute_top_dist(X, centroids)\n",
    "        nearest_centroid_index = np.argmin(dist, axis=1)\n",
    "        return nearest_centroid_index\n",
    "\n",
    "    def _compute_top_dist(self, X, centroid):\n",
    "        \"\"\"Computes the pairwise top. distances between networks and centroids.\"\"\"\n",
    "        return np.dot((X - centroid)**2, self.weight_array)\n",
    "\n",
    "    def _top_interpolation(self, init_centroid, sample_mean,\n",
    "                           top_centroid_birth_set, top_centroid_death_set):\n",
    "        \"\"\"Topological interpolation.\"\"\"\n",
    "        curr = init_centroid\n",
    "        for _ in range(self.max_iter_interp):\n",
    "            # Geometric term gradient\n",
    "            geo_gradient = 2 * (curr - sample_mean)\n",
    "\n",
    "            # Topological term gradient\n",
    "            sorted_birth_ind, sorted_death_ind = self._compute_optimal_matching(\n",
    "                curr)\n",
    "            top_gradient = np.zeros_like(curr)\n",
    "            top_gradient[sorted_birth_ind] = top_centroid_birth_set\n",
    "            top_gradient[sorted_death_ind] = top_centroid_death_set\n",
    "            top_gradient = 2 * (curr - top_gradient)\n",
    "\n",
    "            # Gradient update\n",
    "            curr -= self.learning_rate * (\n",
    "                (1 - self.top_relative_weight) * geo_gradient +\n",
    "                self.top_relative_weight * top_gradient)\n",
    "        return curr\n",
    "\n",
    "    def _compute_optimal_matching(self, adj):\n",
    "        mst, nonmst = self._bd_demomposition(adj)\n",
    "        birth_ind = np.nonzero(mst)\n",
    "        death_ind = np.nonzero(nonmst)\n",
    "        sorted_temp_ind = np.argsort(mst[birth_ind])\n",
    "        sorted_birth_ind = tuple(np.array(birth_ind)[:, sorted_temp_ind])\n",
    "        sorted_temp_ind = np.argsort(nonmst[death_ind])\n",
    "        sorted_death_ind = tuple(np.array(death_ind)[:, sorted_temp_ind])\n",
    "        return sorted_birth_ind, sorted_death_ind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purity_score(labels_true, labels_pred):\n",
    "    mtx = contingency_matrix(labels_true, labels_pred)\n",
    "    return np.sum(np.amax(mtx, axis=0)) / np.sum(mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purity_stats(iterations, top_clust, graphs, labels_true):\n",
    "    scores = np.zeros(iterations)\n",
    "    for i in range(iterations):\n",
    "        labels_pred = top_clust.fit_predict(graphs)\n",
    "        scores[i] = purity_score(labels_pred, labels_true)\n",
    "    \n",
    "    return np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 2\n",
    "max_iter_alt = 300\n",
    "max_iter_interp = 300\n",
    "learning_rate = 0.05\n",
    "\n",
    "iterations = 50\n",
    "labels_true = np.empty(2*n_networks)\n",
    "labels_true[::2] = 0\n",
    "labels_true[1::2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clustering network graphs and computing purity w/ varying topological weights\n",
    "\n",
    "top_weights = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.999]\n",
    "purity = []\n",
    "\n",
    "for w in top_weights:\n",
    "    top_clust = TopClustering(n_clusters, w, max_iter_alt,\n",
    "                                    max_iter_interp,\n",
    "                                    learning_rate)\n",
    "    purity.append(np.asarray(purity_stats(iterations, top_clust, network_graphs, labels_true)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9365 0.907  0.925  0.9025 0.968  0.9675 0.9785 0.9875 0.996  1.\n",
      " 1.    ]\n",
      "[0.06693467 0.10050373 0.08529361 0.12229575 0.07842831 0.06675515\n",
      " 0.07858276 0.05226136 0.02467793 0.         0.        ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "purity = np.asarray(purity)\n",
    "purity_means = purity[:, 0]\n",
    "purity_stds = purity[:, 1]\n",
    "\n",
    "plt.scatter(top_weights, purity_means)\n",
    "plt.xlabel(\"Relative topological weight\")\n",
    "plt.ylabel(\"Mean purity score\")\n",
    "plt.ylim([0, 1.07])\n",
    "plt.errorbar(top_weights, purity_means, purity_stds)\n",
    "\n",
    "print(purity_means)\n",
    "print(purity_stds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
