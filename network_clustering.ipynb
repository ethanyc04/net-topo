{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import minimum_spanning_tree\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load MNIST data\n",
    "\n",
    "train_data = pd.read_csv('./mnist_train.csv', sep=',', header=None)\n",
    "train_labels = train_data[0]\n",
    "train_data = train_data.drop(0, axis=1)\n",
    "\n",
    "test_data = pd.read_csv('./mnist_test.csv', sep=',', header=None)\n",
    "test_labels = test_data[0]\n",
    "test_data = test_data.drop(0, axis=1)\n",
    "\n",
    "#separate data for generating graphs\n",
    "graph_data = train_data.sample(n = 10000, random_state=100)\n",
    "graph_labels = train_labels.sample(n = 10000, random_state=100)\n",
    "train_data = train_data.drop(graph_data.index)\n",
    "train_labels = train_labels.drop(graph_data.index)\n",
    "\n",
    "#convert data to pytorch tensors\n",
    "train_data = torch.FloatTensor(train_data.to_numpy())\n",
    "train_labels = torch.LongTensor(train_labels.to_numpy())\n",
    "test_data = torch.FloatTensor(test_data.to_numpy())\n",
    "graph_data = torch.FloatTensor(graph_data.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(50, 50) Vanilla FCN\n",
    "\n",
    "class Vanilla_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Vanilla_Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.lrelu1 = nn.LeakyReLU(0.01) #default negative slope\n",
    "        self.fc2 = nn.Linear(50, 50)\n",
    "        self.lrelu2 = nn.LeakyReLU(0.01) #default negative slope\n",
    "        self.fc3 = nn.Linear(50, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.lrelu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.lrelu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(net, optimizer, loss, epochs, train_data, train_labels, batch_size):\n",
    "    for i in range(epochs):\n",
    "        for j in range(0, train_data.shape[0], batch_size):\n",
    "            data_minibatch = Variable(train_data[j : j+batch_size])\n",
    "            label_minibatch = Variable(train_labels[j: j+batch_size])\n",
    "            optimizer.zero_grad()\n",
    "            net_out = net(data_minibatch)\n",
    "            net_loss = loss(net_out, label_minibatch)\n",
    "            net_loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(net, test_data, test_labels):\n",
    "    net_out = net(test_data)\n",
    "    test_out = torch.max(net_out.data, 1)[1].numpy()\n",
    "    return np.count_nonzero(test_out==test_labels) / len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ethan\\AppData\\Local\\Temp\\ipykernel_12060\\2732752644.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9487\n",
      "0.9479\n",
      "0.9511\n",
      "0.9519\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[417], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(my_net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate, momentum\u001b[38;5;241m=\u001b[39mmmt)\n\u001b[0;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m---> 14\u001b[0m \u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m cur_accuracy \u001b[38;5;241m=\u001b[39m test_accuracy(my_net, test_data, test_labels)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(cur_accuracy)\n",
      "Cell \u001b[1;32mIn[243], line 7\u001b[0m, in \u001b[0;36mSGD\u001b[1;34m(net, optimizer, loss, epochs, train_data, train_labels, batch_size)\u001b[0m\n\u001b[0;32m      5\u001b[0m label_minibatch \u001b[38;5;241m=\u001b[39m Variable(train_labels[j: j\u001b[38;5;241m+\u001b[39mbatch_size])\n\u001b[0;32m      6\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m----> 7\u001b[0m net_out \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_minibatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m net_loss \u001b[38;5;241m=\u001b[39m loss(net_out, label_minibatch)\n\u001b[0;32m      9\u001b[0m net_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[364], line 13\u001b[0m, in \u001b[0;36mVanilla_Net.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlrelu1(x)\n\u001b[0;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#determine optimal # of epochs for SGD\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 50 #typical value\n",
    "learning_rate = 0.0001 \n",
    "mmt = 0.9 #typical value\n",
    "cur_accuracy = 0\n",
    "prev_accuracy = 0\n",
    "while True:\n",
    "    prev_accuracy = cur_accuracy\n",
    "    my_net = Vanilla_Net()\n",
    "    optimizer = torch.optim.SGD(my_net.parameters(), lr=learning_rate, momentum=mmt)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    SGD(my_net, optimizer, loss, epochs, train_data, train_labels, batch_size)\n",
    "    cur_accuracy = test_accuracy(my_net, test_data, test_labels)\n",
    "    print(cur_accuracy)\n",
    "    if (cur_accuracy <= prev_accuracy-0.005):\n",
    "        break\n",
    "    epochs += 1\n",
    "epochs -= 1\n",
    "print(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above code a few times, it seems like the network typically achieves high accuracy after around 20 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join train and test data\n",
    "\n",
    "train_test_data = torch.cat((train_data, test_data))\n",
    "train_test_labels = torch.cat((train_labels, torch.LongTensor(test_labels.to_numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train vanilla network with train+test data\n",
    "\n",
    "def train_vanilla():\n",
    "    vanilla_net = Vanilla_Net()\n",
    "    optimizer = torch.optim.SGD(vanilla_net.parameters(), lr=0.0001, momentum=0.9)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    SGD(vanilla_net, optimizer, loss, epochs=20, train_data=train_test_data, train_labels=train_test_labels, batch_size=50)\n",
    "    return vanilla_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (50, 50) FCN trained with batch norm\n",
    "\n",
    "class BN_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BN_Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.bn1 = nn.BatchNorm1d(50)\n",
    "        self.lrelu1 = nn.LeakyReLU(0.01)\n",
    "        self.fc2 = nn.Linear(50, 50)\n",
    "        self.bn2 = nn.BatchNorm1d(50)\n",
    "        self.lrelu2 = nn.LeakyReLU(0.01)\n",
    "        self.fc3 = nn.Linear(50, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.lrelu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.lrelu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ethan\\AppData\\Local\\Temp\\ipykernel_24688\\1325394341.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(my_net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate, momentum\u001b[38;5;241m=\u001b[39mmmt)\n\u001b[0;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m---> 14\u001b[0m \u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m cur_accuracy \u001b[38;5;241m=\u001b[39m test_accuracy(my_net, test_data, test_labels)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(cur_accuracy)\n",
      "Cell \u001b[1;32mIn[73], line 9\u001b[0m, in \u001b[0;36mSGD\u001b[1;34m(net, optimizer, loss, epochs, train_data, train_labels, batch_size)\u001b[0m\n\u001b[0;32m      7\u001b[0m net_out \u001b[38;5;241m=\u001b[39m net(data_minibatch)\n\u001b[0;32m      8\u001b[0m net_loss \u001b[38;5;241m=\u001b[39m loss(net_out, label_minibatch)\n\u001b[1;32m----> 9\u001b[0m \u001b[43mnet_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#determine optimal # of epochs for batch_norm_net\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 50 #typical value\n",
    "learning_rate = 0.0001 \n",
    "mmt = 0.9 #typical value\n",
    "cur_accuracy = 0\n",
    "prev_accuracy = 0\n",
    "while True:\n",
    "    prev_accuracy = cur_accuracy\n",
    "    my_net = BN_Net()\n",
    "    optimizer = torch.optim.SGD(my_net.parameters(), lr=learning_rate, momentum=mmt)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    SGD(my_net, optimizer, loss, epochs, train_data, train_labels, batch_size)\n",
    "    cur_accuracy = test_accuracy(my_net, test_data, test_labels)\n",
    "    print(cur_accuracy)\n",
    "    if (cur_accuracy <= prev_accuracy-0.01):\n",
    "        break\n",
    "    epochs += 1\n",
    "epochs -= 1\n",
    "print(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch norm network achieves high accuracy after 20 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train batch norm network\n",
    "\n",
    "def train_batch_norm():\n",
    "    batch_norm_net = BN_Net()\n",
    "    optimizer = torch.optim.SGD(batch_norm_net.parameters(), lr=0.0001, momentum=0.9)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    SGD(batch_norm_net, optimizer, loss, epochs=20, train_data=train_test_data, train_labels=train_test_labels, batch_size=50)\n",
    "    return batch_norm_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ethan\\AppData\\Local\\Temp\\ipykernel_5308\\805662490.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n",
      "C:\\Users\\ethan\\AppData\\Local\\Temp\\ipykernel_5308\\1325394341.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    }
   ],
   "source": [
    "# train n networks for each strategy\n",
    "\n",
    "n_networks = 20\n",
    "vanilla_nets = []\n",
    "batch_norm_nets = []\n",
    "\n",
    "for i in range(n_networks):\n",
    "    vanilla_nets.append(train_vanilla())\n",
    "    batch_norm_nets.append(train_batch_norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract neuron outputs\n",
    "\n",
    "def neuron_values(net, data):\n",
    "    activations = []\n",
    "    def get_activation():\n",
    "        def hook(model, input, output):\n",
    "            activations.append(output.detach())\n",
    "        return hook\n",
    "    \n",
    "    net.lrelu1.register_forward_hook(get_activation())\n",
    "    net.lrelu2.register_forward_hook(get_activation())\n",
    "    net.eval()\n",
    "    net(data)\n",
    "    \n",
    "    activations[0] = activations[0].numpy()\n",
    "    activations[1] = activations[1].numpy()\n",
    "    neurons = np.concatenate((activations[0].T, activations[1].T))\n",
    "    return neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ethan\\AppData\\Local\\Temp\\ipykernel_5308\\805662490.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n",
      "C:\\Users\\ethan\\AppData\\Local\\Temp\\ipykernel_5308\\1325394341.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    }
   ],
   "source": [
    "vanilla_neurons = []\n",
    "batch_norm_neurons = []\n",
    "\n",
    "for i in range(n_networks):\n",
    "    vanilla_neurons.append(neuron_values(vanilla_nets[i], graph_data))\n",
    "    batch_norm_neurons.append(neuron_values(batch_norm_nets[i], graph_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_graph(neurons):\n",
    "    adj_matrix = abs(np.corrcoef(neurons))\n",
    "    np.fill_diagonal(adj_matrix, 0)\n",
    "    return adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split neuron data for one network into n subsets to construct n graphs\n",
    "\n",
    "n_subsets = 100\n",
    "\n",
    "vanilla_subsets = np.array_split(vanilla_neurons[0], n_subsets, 1)\n",
    "batch_norm_subsets = np.array_split(batch_norm_neurons[0], n_subsets, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct network graphs for subsets\n",
    "\n",
    "subset_network_graphs = []\n",
    "\n",
    "for i in range(n_subsets):\n",
    "    subset_network_graphs.append(correlation_graph(vanilla_subsets[i]))\n",
    "    subset_network_graphs.append(correlation_graph(batch_norm_subsets[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct network graphs for distinct networks\n",
    "\n",
    "network_graphs = []\n",
    "\n",
    "for i in range(n_networks):\n",
    "    network_graphs.append(correlation_graph(vanilla_neurons[i]))\n",
    "    network_graphs.append(correlation_graph(batch_norm_neurons[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topological clustering framework\n",
    "\n",
    "class TopClustering:\n",
    "    \"\"\"Topological clustering.\n",
    "    \n",
    "    Attributes:\n",
    "        n_clusters: \n",
    "          The number of clusters.\n",
    "        top_relative_weight:\n",
    "          Relative weight between the geometric and topological terms.\n",
    "          A floating point number between 0 and 1.\n",
    "        max_iter_alt:\n",
    "          Maximum number of iterations for the topological clustering.\n",
    "        max_iter_interp:\n",
    "          Maximum number of iterations for the topological interpolation.\n",
    "        learning_rate:\n",
    "          Learning rate for the topological interpolation.\n",
    "        \n",
    "    Reference:\n",
    "        Songdechakraiwut, Tananun, Bryan M. Krause, Matthew I. Banks, Kirill V. Nourski, and Barry D. Van Veen. \n",
    "        \"Fast topological clustering with Wasserstein distance.\" \n",
    "        International Conference on Learning Representations (ICLR). 2022.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, top_relative_weight, max_iter_alt,\n",
    "                 max_iter_interp, learning_rate):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.top_relative_weight = top_relative_weight\n",
    "        self.max_iter_alt = max_iter_alt\n",
    "        self.max_iter_interp = max_iter_interp\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def fit_predict(self, data):\n",
    "        \"\"\"Computes topological clustering and predicts cluster index for each sample.\n",
    "        \n",
    "            Args:\n",
    "                data:\n",
    "                  Training instances to cluster.\n",
    "                  \n",
    "            Returns:\n",
    "                Cluster index each sample belongs to.\n",
    "        \"\"\"\n",
    "        data = np.asarray(data)\n",
    "        n_node = data.shape[1]\n",
    "        n_edges = math.factorial(n_node) // math.factorial(2) // math.factorial(\n",
    "            n_node - 2)  # n_edges = (n_node choose 2)\n",
    "        n_births = n_node - 1\n",
    "        self.weight_array = np.append(\n",
    "            np.repeat(1 - self.top_relative_weight, n_edges),\n",
    "            np.repeat(self.top_relative_weight, n_edges))\n",
    "\n",
    "        # Networks represented as vectors concatenating geometric and topological info\n",
    "        X = []\n",
    "        for adj in data:\n",
    "            X.append(self._vectorize_geo_top_info(adj))\n",
    "        X = np.asarray(X)\n",
    "\n",
    "        # Random initial condition\n",
    "        self.centroids = X[random.sample(range(X.shape[0]), self.n_clusters)]\n",
    "\n",
    "        # Assign the nearest centroid index to each data point\n",
    "        assigned_centroids = self._get_nearest_centroid(\n",
    "            X[:, None, :], self.centroids[None, :, :])\n",
    "        prev_assigned_centroids = assigned_centroids\n",
    "\n",
    "        for it in range(self.max_iter_alt):\n",
    "            for cluster in range(self.n_clusters):\n",
    "                # Previous iteration centroid\n",
    "                prev_centroid = np.zeros((n_node, n_node))\n",
    "                prev_centroid[np.triu_indices(\n",
    "                    prev_centroid.shape[0],\n",
    "                    k=1)] = self.centroids[cluster][:n_edges]\n",
    "\n",
    "                # Determine data points belonging to each cluster\n",
    "                cluster_members = X[assigned_centroids == cluster]\n",
    "\n",
    "                # Compute the sample mean and top. centroid of the cluster\n",
    "                cc = cluster_members.mean(axis=0)\n",
    "                sample_mean = np.zeros((n_node, n_node))\n",
    "                sample_mean[np.triu_indices(sample_mean.shape[0],\n",
    "                                            k=1)] = cc[:n_edges]\n",
    "                top_centroid = cc[n_edges:]\n",
    "                top_centroid_birth_set = top_centroid[:n_births]\n",
    "                top_centroid_death_set = top_centroid[n_births:]\n",
    "\n",
    "                # Update the centroid\n",
    "                try:\n",
    "                    cluster_centroid = self._top_interpolation(\n",
    "                        prev_centroid, sample_mean, top_centroid_birth_set,\n",
    "                        top_centroid_death_set)\n",
    "                    self.centroids[cluster] = self._vectorize_geo_top_info(\n",
    "                        cluster_centroid)\n",
    "                except:\n",
    "                    print(\n",
    "                        'Error: Possibly due to the learning rate is not within appropriate range.'\n",
    "                    )\n",
    "                    sys.exit(1)\n",
    "\n",
    "            # Update the cluster membership\n",
    "            assigned_centroids = self._get_nearest_centroid(\n",
    "                X[:, None, :], self.centroids[None, :, :])\n",
    "\n",
    "            # Compute and print loss as it is progressively decreasing\n",
    "            loss = self._compute_top_dist(\n",
    "                X, self.centroids[assigned_centroids]).sum() / len(X)\n",
    "            #print('Iteration: %d -> Loss: %f' % (it, loss))\n",
    "\n",
    "            if (prev_assigned_centroids == assigned_centroids).all():\n",
    "                break\n",
    "            else:\n",
    "                prev_assigned_centroids = assigned_centroids\n",
    "        return assigned_centroids\n",
    "\n",
    "    def _vectorize_geo_top_info(self, adj):\n",
    "        birth_set, death_set = self._compute_birth_death_sets(\n",
    "            adj)  # topological info\n",
    "        vec = adj[np.triu_indices(adj.shape[0], k=1)]  # geometric info\n",
    "        return np.concatenate((vec, birth_set, death_set), axis=0)\n",
    "\n",
    "    def _compute_birth_death_sets(self, adj):\n",
    "        \"\"\"Computes birth and death sets of a network.\"\"\"\n",
    "        mst, nonmst = self._bd_demomposition(adj)\n",
    "        birth_ind = np.nonzero(mst)\n",
    "        death_ind = np.nonzero(nonmst)\n",
    "        return np.sort(mst[birth_ind]), np.sort(nonmst[death_ind])\n",
    "\n",
    "    def _bd_demomposition(self, adj):\n",
    "        \"\"\"Birth-death decomposition.\"\"\"\n",
    "        eps = np.nextafter(0, 1)\n",
    "        adj[adj == 0] = eps\n",
    "        adj = np.triu(adj, k=1)\n",
    "        Xcsr = csr_matrix(-adj)\n",
    "        Tcsr = minimum_spanning_tree(Xcsr)\n",
    "        mst = -Tcsr.toarray()  # reverse the negative sign\n",
    "        nonmst = adj - mst\n",
    "        birth_ind = np.nonzero(mst)\n",
    "        return mst, nonmst\n",
    "\n",
    "    def _get_nearest_centroid(self, X, centroids):\n",
    "        \"\"\"Determines cluster membership of data points.\"\"\"\n",
    "        dist = self._compute_top_dist(X, centroids)\n",
    "        nearest_centroid_index = np.argmin(dist, axis=1)\n",
    "        return nearest_centroid_index\n",
    "\n",
    "    def _compute_top_dist(self, X, centroid):\n",
    "        \"\"\"Computes the pairwise top. distances between networks and centroids.\"\"\"\n",
    "        return np.dot((X - centroid)**2, self.weight_array)\n",
    "\n",
    "    def _top_interpolation(self, init_centroid, sample_mean,\n",
    "                           top_centroid_birth_set, top_centroid_death_set):\n",
    "        \"\"\"Topological interpolation.\"\"\"\n",
    "        curr = init_centroid\n",
    "        for _ in range(self.max_iter_interp):\n",
    "            # Geometric term gradient\n",
    "            geo_gradient = 2 * (curr - sample_mean)\n",
    "\n",
    "            # Topological term gradient\n",
    "            sorted_birth_ind, sorted_death_ind = self._compute_optimal_matching(\n",
    "                curr)\n",
    "            top_gradient = np.zeros_like(curr)\n",
    "            top_gradient[sorted_birth_ind] = top_centroid_birth_set\n",
    "            top_gradient[sorted_death_ind] = top_centroid_death_set\n",
    "            top_gradient = 2 * (curr - top_gradient)\n",
    "\n",
    "            # Gradient update\n",
    "            curr -= self.learning_rate * (\n",
    "                (1 - self.top_relative_weight) * geo_gradient +\n",
    "                self.top_relative_weight * top_gradient)\n",
    "        return curr\n",
    "\n",
    "    def _compute_optimal_matching(self, adj):\n",
    "        mst, nonmst = self._bd_demomposition(adj)\n",
    "        birth_ind = np.nonzero(mst)\n",
    "        death_ind = np.nonzero(nonmst)\n",
    "        sorted_temp_ind = np.argsort(mst[birth_ind])\n",
    "        sorted_birth_ind = tuple(np.array(birth_ind)[:, sorted_temp_ind])\n",
    "        sorted_temp_ind = np.argsort(nonmst[death_ind])\n",
    "        sorted_death_ind = tuple(np.array(death_ind)[:, sorted_temp_ind])\n",
    "        return sorted_birth_ind, sorted_death_ind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purity_score(labels_true, labels_pred):\n",
    "    mtx = contingency_matrix(labels_true, labels_pred)\n",
    "    return np.sum(np.amax(mtx, axis=0)) / np.sum(mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purity_stats(iterations, top_clust, graphs, labels_true):\n",
    "    scores = np.zeros(iterations)\n",
    "    for i in range(iterations):\n",
    "        labels_pred = top_clust.fit_predict(graphs)\n",
    "        scores[i] = purity_score(labels_pred, labels_true)\n",
    "    \n",
    "    return np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 2\n",
    "max_iter_alt = 300\n",
    "max_iter_interp = 300\n",
    "learning_rate = 0.05\n",
    "\n",
    "iterations = 100\n",
    "labels_true = np.empty(2*n_networks)\n",
    "labels_true[::2] = 0\n",
    "labels_true[1::2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clustering network graphs and computing purity w/ varying topological weights\n",
    "\n",
    "top_weights = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.999]\n",
    "purity = []\n",
    "\n",
    "for w in top_weights:\n",
    "    top_clust = TopClustering(n_clusters, w, max_iter_alt,\n",
    "                                    max_iter_interp,\n",
    "                                    learning_rate)\n",
    "    purity.append(np.asarray(purity_stats(iterations, top_clust, network_graphs, labels_true)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.92025 0.88625 0.9075  0.9235  0.958   0.9745  0.97825 0.98525 0.996\n",
      " 1.      1.     ]\n",
      "[0.09530838 0.1131578  0.11002841 0.10757672 0.09306449 0.0639316\n",
      " 0.06639418 0.06883813 0.03014963 0.         0.        ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "purity = np.asarray(purity)\n",
    "purity_means = purity[:, 0]\n",
    "purity_stds = purity[:, 1]\n",
    "\n",
    "plt.scatter(top_weights, purity_means)\n",
    "plt.xlabel(\"Relative topological weight\")\n",
    "plt.ylabel(\"Mean purity score\")\n",
    "plt.ylim([0, 1.07])\n",
    "plt.errorbar(top_weights, purity_means, purity_stds)\n",
    "\n",
    "print(purity_means)\n",
    "print(purity_stds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
