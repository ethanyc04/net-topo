{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load MNIST data\n",
    "\n",
    "train_data = pd.read_csv('./mnist_train.csv', sep=',', header=None)\n",
    "train_labels = train_data[0]\n",
    "train_data = train_data.drop(0, axis=1)\n",
    "\n",
    "test_data = pd.read_csv('./mnist_test.csv', sep=',', header=None)\n",
    "test_labels = test_data[0]\n",
    "test_data = test_data.drop(0, axis=1)\n",
    "\n",
    "#separate data for generating graphs\n",
    "graph_data = train_data.sample(n = 10000, random_state=100)\n",
    "graph_labels = train_labels.sample(n = 10000, random_state=100)\n",
    "train_data = train_data.drop(graph_data.index)\n",
    "train_labels = train_labels.drop(graph_data.index)\n",
    "\n",
    "#convert data to pytorch tensors\n",
    "train_data = torch.FloatTensor(train_data.to_numpy())\n",
    "train_labels = torch.LongTensor(train_labels.to_numpy())\n",
    "test_data = torch.FloatTensor(test_data.to_numpy())\n",
    "graph_data = torch.FloatTensor(graph_data.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(50, 50) Vanilla FCN\n",
    "\n",
    "class Vanilla_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Vanilla_Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.lrelu1 = nn.LeakyReLU(0.01) #default negative slope\n",
    "        self.fc2 = nn.Linear(50, 50)\n",
    "        self.lrelu2 = nn.LeakyReLU(0.01) #default negative slope\n",
    "        self.fc3 = nn.Linear(50, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.lrelu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.lrelu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(net, optimizer, loss, epochs, train_data, train_labels, batch_size):\n",
    "    for i in range(epochs):\n",
    "        for j in range(0, train_data.shape[0], batch_size):\n",
    "            data_minibatch = Variable(train_data[j : j+batch_size])\n",
    "            label_minibatch = Variable(train_labels[j: j+batch_size])\n",
    "            optimizer.zero_grad()\n",
    "            net_out = net(data_minibatch)\n",
    "            net_loss = loss(net_out, label_minibatch)\n",
    "            net_loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(net, test_data, test_labels):\n",
    "    net_out = net(test_data)\n",
    "    test_out = torch.max(net_out.data, 1)[1].numpy()\n",
    "    return np.count_nonzero(test_out==test_labels) / len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ethan\\AppData\\Local\\Temp\\ipykernel_12060\\2732752644.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9487\n",
      "0.9479\n",
      "0.9511\n",
      "0.9519\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[417], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(my_net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate, momentum\u001b[38;5;241m=\u001b[39mmmt)\n\u001b[0;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m---> 14\u001b[0m \u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m cur_accuracy \u001b[38;5;241m=\u001b[39m test_accuracy(my_net, test_data, test_labels)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(cur_accuracy)\n",
      "Cell \u001b[1;32mIn[243], line 7\u001b[0m, in \u001b[0;36mSGD\u001b[1;34m(net, optimizer, loss, epochs, train_data, train_labels, batch_size)\u001b[0m\n\u001b[0;32m      5\u001b[0m label_minibatch \u001b[38;5;241m=\u001b[39m Variable(train_labels[j: j\u001b[38;5;241m+\u001b[39mbatch_size])\n\u001b[0;32m      6\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m----> 7\u001b[0m net_out \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_minibatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m net_loss \u001b[38;5;241m=\u001b[39m loss(net_out, label_minibatch)\n\u001b[0;32m      9\u001b[0m net_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[364], line 13\u001b[0m, in \u001b[0;36mVanilla_Net.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlrelu1(x)\n\u001b[0;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#determine optimal # of epochs for SGD\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 50 #typical value\n",
    "learning_rate = 0.0001 \n",
    "mmt = 0.9 #typical value\n",
    "cur_accuracy = 0\n",
    "prev_accuracy = 0\n",
    "while True:\n",
    "    prev_accuracy = cur_accuracy\n",
    "    my_net = Vanilla_Net()\n",
    "    optimizer = torch.optim.SGD(my_net.parameters(), lr=learning_rate, momentum=mmt)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    SGD(my_net, optimizer, loss, epochs, train_data, train_labels, batch_size)\n",
    "    cur_accuracy = test_accuracy(my_net, test_data, test_labels)\n",
    "    print(cur_accuracy)\n",
    "    if (cur_accuracy <= prev_accuracy-0.005):\n",
    "        break\n",
    "    epochs += 1\n",
    "epochs -= 1\n",
    "print(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above code a few times, it seems like the network typically achieves high accuracy after around 20 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join train and test data\n",
    "\n",
    "train_test_data = torch.cat((train_data, test_data))\n",
    "train_test_labels = torch.cat((train_labels, torch.LongTensor(test_labels.to_numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ethan\\AppData\\Local\\Temp\\ipykernel_24688\\805662490.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    }
   ],
   "source": [
    "# train vanilla network with train+test data\n",
    "\n",
    "vanilla_net = Vanilla_Net()\n",
    "optimizer = torch.optim.SGD(vanilla_net.parameters(), lr=0.0001, momentum=0.9)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "SGD(vanilla_net, optimizer, loss, epochs=20, train_data=train_test_data, train_labels=train_test_labels, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (50, 50) FCN trained with batch norm\n",
    "\n",
    "class BN_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BN_Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.bn1 = nn.BatchNorm1d(50)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(50, 50)\n",
    "        self.bn2 = nn.BatchNorm1d(50)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(50, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ethan\\AppData\\Local\\Temp\\ipykernel_12060\\3743631527.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9529\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[213], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(my_net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate, momentum\u001b[38;5;241m=\u001b[39mmmt)\n\u001b[0;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m---> 14\u001b[0m \u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m cur_accuracy \u001b[38;5;241m=\u001b[39m test_accuracy(my_net, test_data, test_labels)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(cur_accuracy)\n",
      "Cell \u001b[1;32mIn[7], line 9\u001b[0m, in \u001b[0;36mSGD\u001b[1;34m(net, optimizer, loss, epochs, train_data, train_labels, batch_size)\u001b[0m\n\u001b[0;32m      7\u001b[0m net_out \u001b[38;5;241m=\u001b[39m net(data_minibatch)\n\u001b[0;32m      8\u001b[0m net_loss \u001b[38;5;241m=\u001b[39m loss(net_out, label_minibatch)\n\u001b[1;32m----> 9\u001b[0m \u001b[43mnet_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#determine optimal # of epochs for batch_norm_net\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 50 #typical value\n",
    "learning_rate = 0.0001 #tested to not cause neuron death\n",
    "mmt = 0.9 #typical value\n",
    "cur_accuracy = 0\n",
    "prev_accuracy = 0\n",
    "while True:\n",
    "    prev_accuracy = cur_accuracy\n",
    "    my_net = BN_Net()\n",
    "    optimizer = torch.optim.SGD(my_net.parameters(), lr=learning_rate, momentum=mmt)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    SGD(my_net, optimizer, loss, epochs, train_data, train_labels, batch_size)\n",
    "    cur_accuracy = test_accuracy(my_net, test_data, test_labels)\n",
    "    print(cur_accuracy)\n",
    "    if (cur_accuracy <= prev_accuracy-0.01):\n",
    "        break\n",
    "    epochs += 1\n",
    "epochs -= 1\n",
    "print(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch norm network achieves high accuracy after 20 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ethan\\AppData\\Local\\Temp\\ipykernel_24688\\2591643034.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    }
   ],
   "source": [
    "# train batch norm network\n",
    "\n",
    "batch_norm_net = BN_Net()\n",
    "optimizer = torch.optim.SGD(batch_norm_net.parameters(), lr=0.0001, momentum=0.9)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "SGD(batch_norm_net, optimizer, loss, epochs=20, train_data=train_test_data, train_labels=train_test_labels, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanilla_neuron_values(net, data):\n",
    "    activations = []\n",
    "    def get_activation():\n",
    "        def hook(model, input, output):\n",
    "            activations.append(output.detach())\n",
    "        return hook\n",
    "    \n",
    "    net.lrelu1.register_forward_hook(get_activation())\n",
    "    net.lrelu2.register_forward_hook(get_activation())\n",
    "    net(data)\n",
    "    \n",
    "    activations[0] = activations[0].numpy()\n",
    "    activations[1] = activations[1].numpy()\n",
    "    neurons = np.concatenate((activations[0].T, activations[1].T))\n",
    "    return neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_neuron_values(net, data):\n",
    "    activations = []\n",
    "    def get_activation():\n",
    "        def hook(model, input, output):\n",
    "            activations.append(output.detach())\n",
    "        return hook\n",
    "    \n",
    "    net.relu1.register_forward_hook(get_activation())\n",
    "    net.relu2.register_forward_hook(get_activation())\n",
    "    net(data)\n",
    "    \n",
    "    activations[0] = activations[0].numpy()\n",
    "    activations[1] = activations[1].numpy()\n",
    "    neurons = np.concatenate((activations[0].T, activations[1].T))\n",
    "    return neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ethan\\AppData\\Local\\Temp\\ipykernel_24688\\805662490.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n",
      "C:\\Users\\ethan\\AppData\\Local\\Temp\\ipykernel_24688\\2591643034.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    }
   ],
   "source": [
    "vanilla_neurons = vanilla_neuron_values(vanilla_net, graph_data)\n",
    "batch_norm_neurons = bn_neuron_values(batch_norm_net, graph_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_graph(neurons):\n",
    "    adj_matrix = abs(np.corrcoef(neurons))\n",
    "    np.fill_diagonal(adj_matrix, 0)\n",
    "    return adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split neuron data into n subsets to construct n graphs\n",
    "\n",
    "n_subsets = 100\n",
    "\n",
    "vanilla_subsets = np.array_split(vanilla_neurons, n_subsets, 1)\n",
    "batch_norm_subsets = np.array_split(batch_norm_neurons, n_subsets, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct network graphs\n",
    "\n",
    "network_graphs = []\n",
    "\n",
    "for i in range(n_subsets):\n",
    "    network_graphs.append(correlation_graph(vanilla_subsets[i]))\n",
    "    network_graphs.append(correlation_graph(batch_norm_subsets[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topological clustering framework\n",
    "\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import minimum_spanning_tree\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "class TopClustering:\n",
    "    \"\"\"Topological clustering.\n",
    "    \n",
    "    Attributes:\n",
    "        n_clusters: \n",
    "          The number of clusters.\n",
    "        top_relative_weight:\n",
    "          Relative weight between the geometric and topological terms.\n",
    "          A floating point number between 0 and 1.\n",
    "        max_iter_alt:\n",
    "          Maximum number of iterations for the topological clustering.\n",
    "        max_iter_interp:\n",
    "          Maximum number of iterations for the topological interpolation.\n",
    "        learning_rate:\n",
    "          Learning rate for the topological interpolation.\n",
    "        \n",
    "    Reference:\n",
    "        Songdechakraiwut, Tananun, Bryan M. Krause, Matthew I. Banks, Kirill V. Nourski, and Barry D. Van Veen. \n",
    "        \"Fast topological clustering with Wasserstein distance.\" \n",
    "        International Conference on Learning Representations (ICLR). 2022.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, top_relative_weight, max_iter_alt,\n",
    "                 max_iter_interp, learning_rate):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.top_relative_weight = top_relative_weight\n",
    "        self.max_iter_alt = max_iter_alt\n",
    "        self.max_iter_interp = max_iter_interp\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def fit_predict(self, data):\n",
    "        \"\"\"Computes topological clustering and predicts cluster index for each sample.\n",
    "        \n",
    "            Args:\n",
    "                data:\n",
    "                  Training instances to cluster.\n",
    "                  \n",
    "            Returns:\n",
    "                Cluster index each sample belongs to.\n",
    "        \"\"\"\n",
    "        data = np.asarray(data)\n",
    "        n_node = data.shape[1]\n",
    "        n_edges = math.factorial(n_node) // math.factorial(2) // math.factorial(\n",
    "            n_node - 2)  # n_edges = (n_node choose 2)\n",
    "        n_births = n_node - 1\n",
    "        self.weight_array = np.append(\n",
    "            np.repeat(1 - self.top_relative_weight, n_edges),\n",
    "            np.repeat(self.top_relative_weight, n_edges))\n",
    "\n",
    "        # Networks represented as vectors concatenating geometric and topological info\n",
    "        X = []\n",
    "        for adj in data:\n",
    "            X.append(self._vectorize_geo_top_info(adj))\n",
    "        X = np.asarray(X)\n",
    "\n",
    "        # Random initial condition\n",
    "        self.centroids = X[random.sample(range(X.shape[0]), self.n_clusters)]\n",
    "\n",
    "        # Assign the nearest centroid index to each data point\n",
    "        assigned_centroids = self._get_nearest_centroid(\n",
    "            X[:, None, :], self.centroids[None, :, :])\n",
    "        prev_assigned_centroids = assigned_centroids\n",
    "\n",
    "        for it in range(self.max_iter_alt):\n",
    "            for cluster in range(self.n_clusters):\n",
    "                # Previous iteration centroid\n",
    "                prev_centroid = np.zeros((n_node, n_node))\n",
    "                prev_centroid[np.triu_indices(\n",
    "                    prev_centroid.shape[0],\n",
    "                    k=1)] = self.centroids[cluster][:n_edges]\n",
    "\n",
    "                # Determine data points belonging to each cluster\n",
    "                cluster_members = X[assigned_centroids == cluster]\n",
    "\n",
    "                # Compute the sample mean and top. centroid of the cluster\n",
    "                cc = cluster_members.mean(axis=0)\n",
    "                sample_mean = np.zeros((n_node, n_node))\n",
    "                sample_mean[np.triu_indices(sample_mean.shape[0],\n",
    "                                            k=1)] = cc[:n_edges]\n",
    "                top_centroid = cc[n_edges:]\n",
    "                top_centroid_birth_set = top_centroid[:n_births]\n",
    "                top_centroid_death_set = top_centroid[n_births:]\n",
    "\n",
    "                # Update the centroid\n",
    "                try:\n",
    "                    cluster_centroid = self._top_interpolation(\n",
    "                        prev_centroid, sample_mean, top_centroid_birth_set,\n",
    "                        top_centroid_death_set)\n",
    "                    self.centroids[cluster] = self._vectorize_geo_top_info(\n",
    "                        cluster_centroid)\n",
    "                except:\n",
    "                    print(\n",
    "                        'Error: Possibly due to the learning rate is not within appropriate range.'\n",
    "                    )\n",
    "                    sys.exit(1)\n",
    "\n",
    "            # Update the cluster membership\n",
    "            assigned_centroids = self._get_nearest_centroid(\n",
    "                X[:, None, :], self.centroids[None, :, :])\n",
    "\n",
    "            # Compute and print loss as it is progressively decreasing\n",
    "            loss = self._compute_top_dist(\n",
    "                X, self.centroids[assigned_centroids]).sum() / len(X)\n",
    "            print('Iteration: %d -> Loss: %f' % (it, loss))\n",
    "\n",
    "            if (prev_assigned_centroids == assigned_centroids).all():\n",
    "                break\n",
    "            else:\n",
    "                prev_assigned_centroids = assigned_centroids\n",
    "        return assigned_centroids\n",
    "\n",
    "    def _vectorize_geo_top_info(self, adj):\n",
    "        birth_set, death_set = self._compute_birth_death_sets(\n",
    "            adj)  # topological info\n",
    "        vec = adj[np.triu_indices(adj.shape[0], k=1)]  # geometric info\n",
    "        return np.concatenate((vec, birth_set, death_set), axis=0)\n",
    "\n",
    "    def _compute_birth_death_sets(self, adj):\n",
    "        \"\"\"Computes birth and death sets of a network.\"\"\"\n",
    "        mst, nonmst = self._bd_demomposition(adj)\n",
    "        birth_ind = np.nonzero(mst)\n",
    "        death_ind = np.nonzero(nonmst)\n",
    "        return np.sort(mst[birth_ind]), np.sort(nonmst[death_ind])\n",
    "\n",
    "    def _bd_demomposition(self, adj):\n",
    "        \"\"\"Birth-death decomposition.\"\"\"\n",
    "        eps = np.nextafter(0, 1)\n",
    "        adj[adj == 0] = eps\n",
    "        adj = np.triu(adj, k=1)\n",
    "        Xcsr = csr_matrix(-adj)\n",
    "        Tcsr = minimum_spanning_tree(Xcsr)\n",
    "        mst = -Tcsr.toarray()  # reverse the negative sign\n",
    "        nonmst = adj - mst\n",
    "        birth_ind = np.nonzero(mst)\n",
    "        return mst, nonmst\n",
    "\n",
    "    def _get_nearest_centroid(self, X, centroids):\n",
    "        \"\"\"Determines cluster membership of data points.\"\"\"\n",
    "        dist = self._compute_top_dist(X, centroids)\n",
    "        nearest_centroid_index = np.argmin(dist, axis=1)\n",
    "        return nearest_centroid_index\n",
    "\n",
    "    def _compute_top_dist(self, X, centroid):\n",
    "        \"\"\"Computes the pairwise top. distances between networks and centroids.\"\"\"\n",
    "        return np.dot((X - centroid)**2, self.weight_array)\n",
    "\n",
    "    def _top_interpolation(self, init_centroid, sample_mean,\n",
    "                           top_centroid_birth_set, top_centroid_death_set):\n",
    "        \"\"\"Topological interpolation.\"\"\"\n",
    "        curr = init_centroid\n",
    "        for _ in range(self.max_iter_interp):\n",
    "            # Geometric term gradient\n",
    "            geo_gradient = 2 * (curr - sample_mean)\n",
    "\n",
    "            # Topological term gradient\n",
    "            sorted_birth_ind, sorted_death_ind = self._compute_optimal_matching(\n",
    "                curr)\n",
    "            top_gradient = np.zeros_like(curr)\n",
    "            top_gradient[sorted_birth_ind] = top_centroid_birth_set\n",
    "            top_gradient[sorted_death_ind] = top_centroid_death_set\n",
    "            top_gradient = 2 * (curr - top_gradient)\n",
    "\n",
    "            # Gradient update\n",
    "            curr -= self.learning_rate * (\n",
    "                (1 - self.top_relative_weight) * geo_gradient +\n",
    "                self.top_relative_weight * top_gradient)\n",
    "        return curr\n",
    "\n",
    "    def _compute_optimal_matching(self, adj):\n",
    "        mst, nonmst = self._bd_demomposition(adj)\n",
    "        birth_ind = np.nonzero(mst)\n",
    "        death_ind = np.nonzero(nonmst)\n",
    "        sorted_temp_ind = np.argsort(mst[birth_ind])\n",
    "        sorted_birth_ind = tuple(np.array(birth_ind)[:, sorted_temp_ind])\n",
    "        sorted_temp_ind = np.argsort(nonmst[death_ind])\n",
    "        sorted_death_ind = tuple(np.array(death_ind)[:, sorted_temp_ind])\n",
    "        return sorted_birth_ind, sorted_death_ind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 -> Loss: 46.732040\n",
      "Iteration: 1 -> Loss: 29.903178\n",
      "[1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1\n",
      " 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0\n",
      " 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1\n",
      " 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0\n",
      " 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1\n",
      " 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "#clustering network graphs\n",
    "\n",
    "n_clusters = 2\n",
    "top_relative_weight = 0\n",
    "max_iter_alt = 300\n",
    "max_iter_interp = 300\n",
    "learning_rate = 0.05\n",
    "\n",
    "labels_pred = TopClustering(n_clusters, top_relative_weight, max_iter_alt,\n",
    "                                max_iter_interp,\n",
    "                                learning_rate).fit_predict(network_graphs)\n",
    "\n",
    "print(labels_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.25912957 0.16489568 ... 0.11437594 0.18770911 0.25446728]\n",
      " [0.25912957 0.         0.22676125 ... 0.33733986 0.08048746 0.28707708]\n",
      " [0.16489568 0.22676125 0.         ... 0.50387375 0.06584253 0.10875955]\n",
      " ...\n",
      " [0.11437594 0.33733986 0.50387375 ... 0.         0.23926586 0.23021397]\n",
      " [0.18770911 0.08048746 0.06584253 ... 0.23926586 0.         0.14453173]\n",
      " [0.25446728 0.28707708 0.10875955 ... 0.23021397 0.14453173 0.        ]]\n",
      "[[0.         0.22596773 0.11430681 ... 0.07174153 0.12941904 0.11712096]\n",
      " [0.22596773 0.         0.33555112 ... 0.0534523  0.15135171 0.11295671]\n",
      " [0.11430681 0.33555112 0.         ... 0.07889193 0.05567947 0.5364643 ]\n",
      " ...\n",
      " [0.07174153 0.0534523  0.07889193 ... 0.         0.27542288 0.13725879]\n",
      " [0.12941904 0.15135171 0.05567947 ... 0.27542288 0.         0.02819753]\n",
      " [0.11712096 0.11295671 0.5364643  ... 0.13725879 0.02819753 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(network_graphs[0])\n",
    "print(network_graphs[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
