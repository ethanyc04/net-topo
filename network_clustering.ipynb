{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import minimum_spanning_tree\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load MNIST data\n",
    "\n",
    "train_data = pd.read_csv('./mnist_train.csv', sep=',', header=None)\n",
    "train_labels = train_data[0]\n",
    "train_data = train_data.drop(0, axis=1)\n",
    "\n",
    "test_data = pd.read_csv('./mnist_test.csv', sep=',', header=None)\n",
    "test_labels = test_data[0]\n",
    "test_data = test_data.drop(0, axis=1)\n",
    "\n",
    "#separate data for generating graphs\n",
    "graph_data = train_data.sample(n = 10000, random_state=100)\n",
    "graph_labels = train_labels.sample(n = 10000, random_state=100)\n",
    "train_data = train_data.drop(graph_data.index)\n",
    "train_labels = train_labels.drop(graph_data.index)\n",
    "\n",
    "#convert data to pytorch tensors\n",
    "train_data = torch.FloatTensor(train_data.to_numpy())\n",
    "train_labels = torch.LongTensor(train_labels.to_numpy())\n",
    "test_data = torch.FloatTensor(test_data.to_numpy())\n",
    "graph_data = torch.FloatTensor(graph_data.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(50, 50) Vanilla FCN\n",
    "\n",
    "class Vanilla_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Vanilla_Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.lrelu1 = nn.LeakyReLU(0.01) #default negative slope\n",
    "        self.fc2 = nn.Linear(50, 50)\n",
    "        self.lrelu2 = nn.LeakyReLU(0.01) #default negative slope\n",
    "        self.fc3 = nn.Linear(50, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.lrelu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.lrelu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(net, optimizer, loss, epochs, train_data, train_labels, batch_size):\n",
    "    for i in range(epochs):\n",
    "        for j in range(0, train_data.shape[0], batch_size):\n",
    "            data_minibatch = Variable(train_data[j : j+batch_size])\n",
    "            label_minibatch = Variable(train_labels[j: j+batch_size])\n",
    "            optimizer.zero_grad()\n",
    "            net_out = net(data_minibatch)\n",
    "            net_loss = loss(net_out, label_minibatch)\n",
    "            net_loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(net, test_data, test_labels):\n",
    "    net_out = net(test_data)\n",
    "    test_out = torch.max(net_out.data, 1)[1].numpy()\n",
    "    return np.count_nonzero(test_out==test_labels) / len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ethan\\AppData\\Local\\Temp\\ipykernel_12060\\2732752644.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9487\n",
      "0.9479\n",
      "0.9511\n",
      "0.9519\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[417], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(my_net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate, momentum\u001b[38;5;241m=\u001b[39mmmt)\n\u001b[0;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m---> 14\u001b[0m \u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m cur_accuracy \u001b[38;5;241m=\u001b[39m test_accuracy(my_net, test_data, test_labels)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(cur_accuracy)\n",
      "Cell \u001b[1;32mIn[243], line 7\u001b[0m, in \u001b[0;36mSGD\u001b[1;34m(net, optimizer, loss, epochs, train_data, train_labels, batch_size)\u001b[0m\n\u001b[0;32m      5\u001b[0m label_minibatch \u001b[38;5;241m=\u001b[39m Variable(train_labels[j: j\u001b[38;5;241m+\u001b[39mbatch_size])\n\u001b[0;32m      6\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m----> 7\u001b[0m net_out \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_minibatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m net_loss \u001b[38;5;241m=\u001b[39m loss(net_out, label_minibatch)\n\u001b[0;32m      9\u001b[0m net_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[364], line 13\u001b[0m, in \u001b[0;36mVanilla_Net.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlrelu1(x)\n\u001b[0;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#determine optimal # of epochs for SGD\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 50 #typical value\n",
    "learning_rate = 0.0001 \n",
    "mmt = 0.9 #typical value\n",
    "cur_accuracy = 0\n",
    "prev_accuracy = 0\n",
    "while True:\n",
    "    prev_accuracy = cur_accuracy\n",
    "    my_net = Vanilla_Net()\n",
    "    optimizer = torch.optim.SGD(my_net.parameters(), lr=learning_rate, momentum=mmt)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    SGD(my_net, optimizer, loss, epochs, train_data, train_labels, batch_size)\n",
    "    cur_accuracy = test_accuracy(my_net, test_data, test_labels)\n",
    "    print(cur_accuracy)\n",
    "    if (cur_accuracy <= prev_accuracy-0.005):\n",
    "        break\n",
    "    epochs += 1\n",
    "epochs -= 1\n",
    "print(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above code a few times, it seems like the network typically achieves high accuracy after around 20 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join train and test data\n",
    "\n",
    "train_test_data = torch.cat((train_data, test_data))\n",
    "train_test_labels = torch.cat((train_labels, torch.LongTensor(test_labels.to_numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train vanilla network with train+test data\n",
    "\n",
    "def train_vanilla():\n",
    "    vanilla_net = Vanilla_Net()\n",
    "    optimizer = torch.optim.SGD(vanilla_net.parameters(), lr=0.0001, momentum=0.9)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    SGD(vanilla_net, optimizer, loss, epochs=20, train_data=train_test_data, train_labels=train_test_labels, batch_size=50)\n",
    "    return vanilla_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (50, 50) FCN trained with batch norm\n",
    "\n",
    "class BN_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BN_Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.bn1 = nn.BatchNorm1d(50)\n",
    "        self.lrelu1 = nn.LeakyReLU(0.01)\n",
    "        self.fc2 = nn.Linear(50, 50)\n",
    "        self.bn2 = nn.BatchNorm1d(50)\n",
    "        self.lrelu2 = nn.LeakyReLU(0.01)\n",
    "        self.fc3 = nn.Linear(50, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.lrelu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.lrelu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ethan\\AppData\\Local\\Temp\\ipykernel_24688\\1325394341.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(my_net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate, momentum\u001b[38;5;241m=\u001b[39mmmt)\n\u001b[0;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m---> 14\u001b[0m \u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m cur_accuracy \u001b[38;5;241m=\u001b[39m test_accuracy(my_net, test_data, test_labels)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(cur_accuracy)\n",
      "Cell \u001b[1;32mIn[73], line 9\u001b[0m, in \u001b[0;36mSGD\u001b[1;34m(net, optimizer, loss, epochs, train_data, train_labels, batch_size)\u001b[0m\n\u001b[0;32m      7\u001b[0m net_out \u001b[38;5;241m=\u001b[39m net(data_minibatch)\n\u001b[0;32m      8\u001b[0m net_loss \u001b[38;5;241m=\u001b[39m loss(net_out, label_minibatch)\n\u001b[1;32m----> 9\u001b[0m \u001b[43mnet_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#determine optimal # of epochs for batch_norm_net\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 50 #typical value\n",
    "learning_rate = 0.0001 #tested to not cause neuron death\n",
    "mmt = 0.9 #typical value\n",
    "cur_accuracy = 0\n",
    "prev_accuracy = 0\n",
    "while True:\n",
    "    prev_accuracy = cur_accuracy\n",
    "    my_net = BN_Net()\n",
    "    optimizer = torch.optim.SGD(my_net.parameters(), lr=learning_rate, momentum=mmt)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    SGD(my_net, optimizer, loss, epochs, train_data, train_labels, batch_size)\n",
    "    cur_accuracy = test_accuracy(my_net, test_data, test_labels)\n",
    "    print(cur_accuracy)\n",
    "    if (cur_accuracy <= prev_accuracy-0.01):\n",
    "        break\n",
    "    epochs += 1\n",
    "epochs -= 1\n",
    "print(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch norm network achieves high accuracy after 20 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train batch norm network\n",
    "\n",
    "def train_batch_norm():\n",
    "    batch_norm_net = BN_Net()\n",
    "    optimizer = torch.optim.SGD(batch_norm_net.parameters(), lr=0.0001, momentum=0.9)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    SGD(batch_norm_net, optimizer, loss, epochs=20, train_data=train_test_data, train_labels=train_test_labels, batch_size=50)\n",
    "    return batch_norm_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ethan\\AppData\\Local\\Temp\\ipykernel_24688\\805662490.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n",
      "C:\\Users\\ethan\\AppData\\Local\\Temp\\ipykernel_24688\\1325394341.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    }
   ],
   "source": [
    "# train n networks for each strategy\n",
    "\n",
    "n_networks = 20\n",
    "vanilla_nets = []\n",
    "batch_norm_nets = []\n",
    "\n",
    "for i in range(n_networks):\n",
    "    vanilla_nets.append(train_vanilla())\n",
    "    batch_norm_nets.append(train_batch_norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract neuron outputs\n",
    "\n",
    "def neuron_values(net, data):\n",
    "    activations = []\n",
    "    def get_activation():\n",
    "        def hook(model, input, output):\n",
    "            activations.append(output.detach())\n",
    "        return hook\n",
    "    \n",
    "    net.lrelu1.register_forward_hook(get_activation())\n",
    "    net.lrelu2.register_forward_hook(get_activation())\n",
    "    net(data)\n",
    "    \n",
    "    activations[0] = activations[0].numpy()\n",
    "    activations[1] = activations[1].numpy()\n",
    "    neurons = np.concatenate((activations[0].T, activations[1].T))\n",
    "    return neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ethan\\AppData\\Local\\Temp\\ipykernel_24688\\805662490.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n",
      "C:\\Users\\ethan\\AppData\\Local\\Temp\\ipykernel_24688\\1325394341.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    }
   ],
   "source": [
    "vanilla_neurons = []\n",
    "batch_norm_neurons = []\n",
    "\n",
    "for i in range(n_networks):\n",
    "    vanilla_neurons.append(neuron_values(vanilla_nets[i], graph_data))\n",
    "    batch_norm_neurons.append(neuron_values(batch_norm_nets[i], graph_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_graph(neurons):\n",
    "    adj_matrix = abs(np.corrcoef(neurons))\n",
    "    np.fill_diagonal(adj_matrix, 0)\n",
    "    return adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split neuron data for one network into n subsets to construct n graphs\n",
    "\n",
    "n_subsets = 100\n",
    "\n",
    "vanilla_subsets = np.array_split(vanilla_neurons[0], n_subsets, 1)\n",
    "batch_norm_subsets = np.array_split(batch_norm_neurons[0], n_subsets, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct network graphs for subsets\n",
    "\n",
    "subset_network_graphs = []\n",
    "\n",
    "for i in range(n_subsets):\n",
    "    subset_network_graphs.append(correlation_graph(vanilla_subsets[i]))\n",
    "    subset_network_graphs.append(correlation_graph(batch_norm_subsets[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct network graphs for distinct networks\n",
    "\n",
    "network_graphs = []\n",
    "\n",
    "for i in range(n_networks):\n",
    "    network_graphs.append(correlation_graph(vanilla_neurons[i]))\n",
    "    network_graphs.append(correlation_graph(batch_norm_neurons[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topological clustering framework\n",
    "\n",
    "class TopClustering:\n",
    "    \"\"\"Topological clustering.\n",
    "    \n",
    "    Attributes:\n",
    "        n_clusters: \n",
    "          The number of clusters.\n",
    "        top_relative_weight:\n",
    "          Relative weight between the geometric and topological terms.\n",
    "          A floating point number between 0 and 1.\n",
    "        max_iter_alt:\n",
    "          Maximum number of iterations for the topological clustering.\n",
    "        max_iter_interp:\n",
    "          Maximum number of iterations for the topological interpolation.\n",
    "        learning_rate:\n",
    "          Learning rate for the topological interpolation.\n",
    "        \n",
    "    Reference:\n",
    "        Songdechakraiwut, Tananun, Bryan M. Krause, Matthew I. Banks, Kirill V. Nourski, and Barry D. Van Veen. \n",
    "        \"Fast topological clustering with Wasserstein distance.\" \n",
    "        International Conference on Learning Representations (ICLR). 2022.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, top_relative_weight, max_iter_alt,\n",
    "                 max_iter_interp, learning_rate):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.top_relative_weight = top_relative_weight\n",
    "        self.max_iter_alt = max_iter_alt\n",
    "        self.max_iter_interp = max_iter_interp\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def fit_predict(self, data):\n",
    "        \"\"\"Computes topological clustering and predicts cluster index for each sample.\n",
    "        \n",
    "            Args:\n",
    "                data:\n",
    "                  Training instances to cluster.\n",
    "                  \n",
    "            Returns:\n",
    "                Cluster index each sample belongs to.\n",
    "        \"\"\"\n",
    "        data = np.asarray(data)\n",
    "        n_node = data.shape[1]\n",
    "        n_edges = math.factorial(n_node) // math.factorial(2) // math.factorial(\n",
    "            n_node - 2)  # n_edges = (n_node choose 2)\n",
    "        n_births = n_node - 1\n",
    "        self.weight_array = np.append(\n",
    "            np.repeat(1 - self.top_relative_weight, n_edges),\n",
    "            np.repeat(self.top_relative_weight, n_edges))\n",
    "\n",
    "        # Networks represented as vectors concatenating geometric and topological info\n",
    "        X = []\n",
    "        for adj in data:\n",
    "            X.append(self._vectorize_geo_top_info(adj))\n",
    "        X = np.asarray(X)\n",
    "\n",
    "        # Random initial condition\n",
    "        self.centroids = X[random.sample(range(X.shape[0]), self.n_clusters)]\n",
    "\n",
    "        # Assign the nearest centroid index to each data point\n",
    "        assigned_centroids = self._get_nearest_centroid(\n",
    "            X[:, None, :], self.centroids[None, :, :])\n",
    "        prev_assigned_centroids = assigned_centroids\n",
    "\n",
    "        for it in range(self.max_iter_alt):\n",
    "            for cluster in range(self.n_clusters):\n",
    "                # Previous iteration centroid\n",
    "                prev_centroid = np.zeros((n_node, n_node))\n",
    "                prev_centroid[np.triu_indices(\n",
    "                    prev_centroid.shape[0],\n",
    "                    k=1)] = self.centroids[cluster][:n_edges]\n",
    "\n",
    "                # Determine data points belonging to each cluster\n",
    "                cluster_members = X[assigned_centroids == cluster]\n",
    "\n",
    "                # Compute the sample mean and top. centroid of the cluster\n",
    "                cc = cluster_members.mean(axis=0)\n",
    "                sample_mean = np.zeros((n_node, n_node))\n",
    "                sample_mean[np.triu_indices(sample_mean.shape[0],\n",
    "                                            k=1)] = cc[:n_edges]\n",
    "                top_centroid = cc[n_edges:]\n",
    "                top_centroid_birth_set = top_centroid[:n_births]\n",
    "                top_centroid_death_set = top_centroid[n_births:]\n",
    "\n",
    "                # Update the centroid\n",
    "                try:\n",
    "                    cluster_centroid = self._top_interpolation(\n",
    "                        prev_centroid, sample_mean, top_centroid_birth_set,\n",
    "                        top_centroid_death_set)\n",
    "                    self.centroids[cluster] = self._vectorize_geo_top_info(\n",
    "                        cluster_centroid)\n",
    "                except:\n",
    "                    print(\n",
    "                        'Error: Possibly due to the learning rate is not within appropriate range.'\n",
    "                    )\n",
    "                    sys.exit(1)\n",
    "\n",
    "            # Update the cluster membership\n",
    "            assigned_centroids = self._get_nearest_centroid(\n",
    "                X[:, None, :], self.centroids[None, :, :])\n",
    "\n",
    "            # Compute and print loss as it is progressively decreasing\n",
    "            loss = self._compute_top_dist(\n",
    "                X, self.centroids[assigned_centroids]).sum() / len(X)\n",
    "            #print('Iteration: %d -> Loss: %f' % (it, loss))\n",
    "\n",
    "            if (prev_assigned_centroids == assigned_centroids).all():\n",
    "                break\n",
    "            else:\n",
    "                prev_assigned_centroids = assigned_centroids\n",
    "        return assigned_centroids\n",
    "\n",
    "    def _vectorize_geo_top_info(self, adj):\n",
    "        birth_set, death_set = self._compute_birth_death_sets(\n",
    "            adj)  # topological info\n",
    "        vec = adj[np.triu_indices(adj.shape[0], k=1)]  # geometric info\n",
    "        return np.concatenate((vec, birth_set, death_set), axis=0)\n",
    "\n",
    "    def _compute_birth_death_sets(self, adj):\n",
    "        \"\"\"Computes birth and death sets of a network.\"\"\"\n",
    "        mst, nonmst = self._bd_demomposition(adj)\n",
    "        birth_ind = np.nonzero(mst)\n",
    "        death_ind = np.nonzero(nonmst)\n",
    "        return np.sort(mst[birth_ind]), np.sort(nonmst[death_ind])\n",
    "\n",
    "    def _bd_demomposition(self, adj):\n",
    "        \"\"\"Birth-death decomposition.\"\"\"\n",
    "        eps = np.nextafter(0, 1)\n",
    "        adj[adj == 0] = eps\n",
    "        adj = np.triu(adj, k=1)\n",
    "        Xcsr = csr_matrix(-adj)\n",
    "        Tcsr = minimum_spanning_tree(Xcsr)\n",
    "        mst = -Tcsr.toarray()  # reverse the negative sign\n",
    "        nonmst = adj - mst\n",
    "        birth_ind = np.nonzero(mst)\n",
    "        return mst, nonmst\n",
    "\n",
    "    def _get_nearest_centroid(self, X, centroids):\n",
    "        \"\"\"Determines cluster membership of data points.\"\"\"\n",
    "        dist = self._compute_top_dist(X, centroids)\n",
    "        nearest_centroid_index = np.argmin(dist, axis=1)\n",
    "        return nearest_centroid_index\n",
    "\n",
    "    def _compute_top_dist(self, X, centroid):\n",
    "        \"\"\"Computes the pairwise top. distances between networks and centroids.\"\"\"\n",
    "        return np.dot((X - centroid)**2, self.weight_array)\n",
    "\n",
    "    def _top_interpolation(self, init_centroid, sample_mean,\n",
    "                           top_centroid_birth_set, top_centroid_death_set):\n",
    "        \"\"\"Topological interpolation.\"\"\"\n",
    "        curr = init_centroid\n",
    "        for _ in range(self.max_iter_interp):\n",
    "            # Geometric term gradient\n",
    "            geo_gradient = 2 * (curr - sample_mean)\n",
    "\n",
    "            # Topological term gradient\n",
    "            sorted_birth_ind, sorted_death_ind = self._compute_optimal_matching(\n",
    "                curr)\n",
    "            top_gradient = np.zeros_like(curr)\n",
    "            top_gradient[sorted_birth_ind] = top_centroid_birth_set\n",
    "            top_gradient[sorted_death_ind] = top_centroid_death_set\n",
    "            top_gradient = 2 * (curr - top_gradient)\n",
    "\n",
    "            # Gradient update\n",
    "            curr -= self.learning_rate * (\n",
    "                (1 - self.top_relative_weight) * geo_gradient +\n",
    "                self.top_relative_weight * top_gradient)\n",
    "        return curr\n",
    "\n",
    "    def _compute_optimal_matching(self, adj):\n",
    "        mst, nonmst = self._bd_demomposition(adj)\n",
    "        birth_ind = np.nonzero(mst)\n",
    "        death_ind = np.nonzero(nonmst)\n",
    "        sorted_temp_ind = np.argsort(mst[birth_ind])\n",
    "        sorted_birth_ind = tuple(np.array(birth_ind)[:, sorted_temp_ind])\n",
    "        sorted_temp_ind = np.argsort(nonmst[death_ind])\n",
    "        sorted_death_ind = tuple(np.array(death_ind)[:, sorted_temp_ind])\n",
    "        return sorted_birth_ind, sorted_death_ind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purity_score(labels_true, labels_pred):\n",
    "    mtx = contingency_matrix(labels_true, labels_pred)\n",
    "    return np.sum(np.amax(mtx, axis=0)) / np.sum(mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purity_stats(iterations, top_clust, graphs, labels_true):\n",
    "    scores = np.zeros(iterations)\n",
    "    for i in range(iterations):\n",
    "        labels_pred = top_clust.fit_predict(graphs)\n",
    "        scores[i] = purity_score(labels_pred, labels_true)\n",
    "    \n",
    "    return np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 2\n",
    "max_iter_alt = 300\n",
    "max_iter_interp = 300\n",
    "learning_rate = 0.05\n",
    "\n",
    "iterations = 50\n",
    "labels_true = np.empty(2*n_networks)\n",
    "labels_true[::2] = 0\n",
    "labels_true[1::2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clustering network graphs and computing purity w/ varying topological weights\n",
    "\n",
    "top_weights = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.999]\n",
    "purity = []\n",
    "\n",
    "for w in top_weights:\n",
    "    top_clust = TopClustering(n_clusters, w, max_iter_alt,\n",
    "                                    max_iter_interp,\n",
    "                                    learning_rate)\n",
    "    purity.append(np.asarray(purity_stats(iterations, top_clust, network_graphs, labels_true)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ErrorbarContainer object of 3 artists>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "purity = np.asarray(purity)\n",
    "purity_means = purity[:, 0]\n",
    "purity_stds = purity[:, 1]\n",
    "\n",
    "plt.scatter(top_weights, purity_means)\n",
    "plt.xlabel(\"Relative topological weight\")\n",
    "plt.ylabel(\"Mean purity score\")\n",
    "plt.ylim([0, 1.07])\n",
    "plt.errorbar(top_weights, purity_means, purity_stds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
