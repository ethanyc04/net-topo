{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load MNIST data\n",
    "\n",
    "train_data = pd.read_csv('./mnist_train.csv', sep=',', header=None)\n",
    "train_labels = train_data[0]\n",
    "train_data = train_data.drop(0, axis=1)\n",
    "\n",
    "test_data = pd.read_csv('./mnist_test.csv', sep=',', header=None)\n",
    "test_labels = test_data[0]\n",
    "test_data = test_data.drop(0, axis=1)\n",
    "\n",
    "#separate data for generating graphs\n",
    "graph_data = train_data.sample(n = 10000, random_state=100)\n",
    "graph_labels = train_labels.sample(n = 10000, random_state=100)\n",
    "train_data = train_data.drop(graph_data.index)\n",
    "train_labels = train_labels.drop(graph_data.index)\n",
    "\n",
    "#convert data to pytorch tensors\n",
    "train_data = torch.FloatTensor(train_data.to_numpy())\n",
    "train_labels = torch.LongTensor(train_labels.to_numpy())\n",
    "test_data = torch.FloatTensor(test_data.to_numpy())\n",
    "graph_data = torch.FloatTensor(graph_data.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(300, 100) Vanilla FCN\n",
    "\n",
    "class Vanilla_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Vanilla_Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 300)\n",
    "        self.fc2 = nn.Linear(300, 100)\n",
    "        self.fc3 = nn.Linear(100, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(net, optimizer, loss, epochs, train_data, train_labels, batch_size):\n",
    "    for i in range(epochs):\n",
    "        for j in range(0, train_data.shape[0], batch_size):\n",
    "            data_minibatch = Variable(train_data[j : j+batch_size])\n",
    "            label_minibatch = Variable(train_labels[j: j+batch_size])\n",
    "            optimizer.zero_grad()\n",
    "            net_out = net(data_minibatch)\n",
    "            net_loss = loss(net_out, label_minibatch)\n",
    "            net_loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(net, test_data, test_labels):\n",
    "    net_out = net(test_data)\n",
    "    test_out = torch.max(net_out.data, 1)[1].numpy()\n",
    "    return np.count_nonzero(test_out==test_labels) / len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ethan\\AppData\\Local\\Temp\\ipykernel_15756\\3218874815.py:14: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9178\n",
      "0.9513\n",
      "0.9497\n",
      "0.956\n",
      "0.963\n",
      "0.9673\n",
      "0.9586\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "#determine optimal # of epochs for SGD\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 20 #typical value\n",
    "learning_rate = 0.001 #default value\n",
    "mmt = 0.9 #typical value\n",
    "cur_accuracy = 0\n",
    "prev_accuracy = 0\n",
    "while True:\n",
    "    prev_accuracy = cur_accuracy\n",
    "    my_net = Vanilla_Net()\n",
    "    optimizer = torch.optim.SGD(my_net.parameters(), lr=learning_rate, momentum=mmt)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    SGD(my_net, optimizer, loss, epochs, train_data, train_labels, batch_size)\n",
    "    cur_accuracy = test_accuracy(my_net, test_data, test_labels)\n",
    "    print(cur_accuracy)\n",
    "    if (cur_accuracy <= prev_accuracy-0.005):\n",
    "        break\n",
    "    epochs += 1\n",
    "epochs -= 1\n",
    "print(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above code a few times, it seems like the network typically achieves maximum accuracy after around 5 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join train and test data\n",
    "\n",
    "train_test_data = torch.cat((train_data, test_data))\n",
    "train_test_labels = torch.cat((train_labels, torch.LongTensor(test_labels.to_numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ethan\\AppData\\Local\\Temp\\ipykernel_15756\\3218874815.py:14: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    }
   ],
   "source": [
    "# train vanilla network with train+test data\n",
    "\n",
    "vanilla_net = Vanilla_Net()\n",
    "optimizer = torch.optim.SGD(vanilla_net.parameters(), lr=0.001, momentum=0.9)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "SGD(vanilla_net, optimizer, loss, epochs=5, train_data=train_test_data, train_labels=train_test_labels, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(300, 100) FCN trained with dropout\n",
    "\n",
    "p = 0.5 #same as paper\n",
    "\n",
    "class Dropout_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Dropout_Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 300)\n",
    "        self.fc2 = nn.Linear(300, 100)\n",
    "        self.fc3 = nn.Linear(100, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #x = F.dropout(x, p)\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ethan\\AppData\\Local\\Temp\\ipykernel_15756\\2299957896.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8501\n",
      "0.8736\n",
      "0.8971\n",
      "0.9055\n",
      "0.9127\n",
      "0.9146\n",
      "0.9201\n",
      "0.9263\n",
      "0.9291\n",
      "0.9344\n",
      "0.9218\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "#determine optimal # of epochs for dropout_net\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 20 #typical value\n",
    "learning_rate = 0.001 #default value\n",
    "mmt = 0.9 #typical value\n",
    "cur_accuracy = 0\n",
    "prev_accuracy = 0\n",
    "while True:\n",
    "    prev_accuracy = cur_accuracy\n",
    "    my_net = Dropout_Net()\n",
    "    optimizer = torch.optim.SGD(my_net.parameters(), lr=learning_rate, momentum=mmt)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    SGD(my_net, optimizer, loss, epochs, train_data, train_labels, batch_size)\n",
    "    cur_accuracy = test_accuracy(my_net, test_data, test_labels)\n",
    "    print(cur_accuracy)\n",
    "    if (cur_accuracy <= prev_accuracy-0.01):\n",
    "        break\n",
    "    epochs += 1\n",
    "epochs -= 1\n",
    "print(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes around 10 epochs to train the dropout network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ethan\\AppData\\Local\\Temp\\ipykernel_15756\\2299957896.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    }
   ],
   "source": [
    "# train dropout network\n",
    "\n",
    "dropout_net = Dropout_Net()\n",
    "optimizer = torch.optim.SGD(dropout_net.parameters(), lr=0.001, momentum=0.9)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "SGD(dropout_net, optimizer, loss, epochs=10, train_data=train_test_data, train_labels=train_test_labels, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (300, 100) FCN trained with batch norm\n",
    "\n",
    "class BN_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BN_Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 300)\n",
    "        self.bn1 = nn.BatchNorm1d(300)\n",
    "        self.fc2 = nn.Linear(300, 100)\n",
    "        self.bn2 = nn.BatchNorm1d(100)\n",
    "        self.fc3 = nn.Linear(100, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ethan\\AppData\\Local\\Temp\\ipykernel_15756\\1952347635.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9587\n",
      "0.9702\n",
      "0.9726\n",
      "0.9743\n",
      "0.9769\n",
      "0.9767\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[95], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(my_net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate, momentum\u001b[38;5;241m=\u001b[39mmmt)\n\u001b[0;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m---> 14\u001b[0m \u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m cur_accuracy \u001b[38;5;241m=\u001b[39m test_accuracy(my_net, test_data, test_labels)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(cur_accuracy)\n",
      "Cell \u001b[1;32mIn[38], line 7\u001b[0m, in \u001b[0;36mSGD\u001b[1;34m(net, optimizer, loss, epochs, train_data, train_labels, batch_size)\u001b[0m\n\u001b[0;32m      5\u001b[0m label_minibatch \u001b[38;5;241m=\u001b[39m Variable(train_labels[j: j\u001b[38;5;241m+\u001b[39mbatch_size])\n\u001b[0;32m      6\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m----> 7\u001b[0m net_out \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_minibatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m net_loss \u001b[38;5;241m=\u001b[39m loss(net_out, label_minibatch)\n\u001b[0;32m      9\u001b[0m net_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[93], line 13\u001b[0m, in \u001b[0;36mBN_Net.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[0;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#determine optimal # of epochs for dropout_net\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 20 #typical value\n",
    "learning_rate = 0.001 #default value\n",
    "mmt = 0.9 #typical value\n",
    "cur_accuracy = 0\n",
    "prev_accuracy = 0\n",
    "while True:\n",
    "    prev_accuracy = cur_accuracy\n",
    "    my_net = BN_Net()\n",
    "    optimizer = torch.optim.SGD(my_net.parameters(), lr=learning_rate, momentum=mmt)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    SGD(my_net, optimizer, loss, epochs, train_data, train_labels, batch_size)\n",
    "    cur_accuracy = test_accuracy(my_net, test_data, test_labels)\n",
    "    print(cur_accuracy)\n",
    "    if (cur_accuracy <= prev_accuracy-0.01):\n",
    "        break\n",
    "    epochs += 1\n",
    "epochs -= 1\n",
    "print(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch norm network achieves high accuracy after 5 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ethan\\AppData\\Local\\Temp\\ipykernel_15756\\1952347635.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    }
   ],
   "source": [
    "# train batch norm network\n",
    "\n",
    "batch_norm_net = BN_Net()\n",
    "optimizer = torch.optim.SGD(batch_norm_net.parameters(), lr=0.001, momentum=0.9)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "SGD(batch_norm_net, optimizer, loss, epochs=10, train_data=train_test_data, train_labels=train_test_labels, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split graph data into 10\n",
    "\n",
    "graph_data_subsets = np.array_split(graph_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_values(net, data):\n",
    "    activations = []\n",
    "    def get_activation():\n",
    "        def hook(model, input, output):\n",
    "            activations.append(output.detach())\n",
    "        return hook\n",
    "    \n",
    "    net.fc1.register_forward_hook(get_activation())\n",
    "    net.fc2.register_forward_hook(get_activation())\n",
    "    net(data)\n",
    "\n",
    "    activations[0] = ((abs(activations[0]) + activations[0])/2).numpy()\n",
    "    activations[1] = ((abs(activations[1]) + activations[1])/2).numpy()\n",
    "    neurons = np.concatenate((activations[0].T, activations[1].T))\n",
    "    return neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ethan\\AppData\\Local\\Temp\\ipykernel_15756\\3218874815.py:14: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n",
      "C:\\Users\\ethan\\AppData\\Local\\Temp\\ipykernel_15756\\2299957896.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n",
      "C:\\Users\\ethan\\AppData\\Local\\Temp\\ipykernel_15756\\1952347635.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vanilla_neurons = neuron_values(vanilla_net, graph_data)\n",
    "dropout_neurons = neuron_values(dropout_net, graph_data)\n",
    "batch_norm_neurons = neuron_values(batch_norm_net, graph_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_graph(neurons):\n",
    "    n = len(neurons)\n",
    "    adj_matrix = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            adj_matrix[i][j] = abs(scipy.stats.pearsonr(neurons[i], neurons[j])[0])\n",
    "    return neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split neuron data into 10 subsets to construct 10 graphs\n",
    "vanilla_subsets = np.array_split(vanilla_neurons, 10, 1)\n",
    "dropout_subsets = np.array_split(dropout_neurons, 10, 1)\n",
    "batch_norm_subsets = np.array_split(batch_norm_neurons, 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ethan\\AppData\\Local\\Temp\\ipykernel_15756\\3043669417.py:6: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  adj_matrix[i][j] = abs(scipy.stats.pearsonr(neurons[i], neurons[j])[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.        0.        0.      ...   0.        0.        0.     ]\n",
      " [  0.        0.        0.      ... 316.6242    0.        0.     ]\n",
      " [ 66.8235    0.        0.      ...   0.        0.      199.77635]\n",
      " ...\n",
      " [  0.        0.        0.      ...   0.        0.        0.     ]\n",
      " [  0.        0.        0.      ...   8.08379   0.        0.     ]\n",
      " [  0.        0.        0.      ...   0.        0.        0.     ]]\n"
     ]
    }
   ],
   "source": [
    "network_graphs = []\n",
    "\n",
    "for i in range(10):\n",
    "    network_graphs.append(correlation_graph(vanilla_subsets[i]))\n",
    "    network_graphs.append(correlation_graph(dropout_subsets[i]))\n",
    "    network_graphs.append(correlation_graph(batch_norm_subsets[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(network_graphs[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
